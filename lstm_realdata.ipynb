{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_realdata.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0kiaA_eY6lMX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "229308b4-23b1-4dbc-f228-fd8d0998c63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from string import punctuation\n",
        "import string\n",
        "import copy\n",
        "import torch\n",
        "from torch._C import dtype\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "SDXwWSaY7syY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/review_head.csv')\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "746PrMXq7WMF",
        "outputId": "5c0da2ec-9fbd-4262-ed48-ffba0042c679"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Id', 'ProductId', 'UserId', 'ProfileName', 'HelpfulnessNumerator',\n",
            "       'HelpfulnessDenominator', 'Score', 'Time', 'Summary', 'Text'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop useless columns\n",
        "df.drop(\n",
        "    columns=[\n",
        "        \"Id\",\n",
        "        \"ProductId\",\n",
        "        \"UserId\",\n",
        "        \"ProfileName\",\n",
        "        \"HelpfulnessNumerator\",\n",
        "        \"HelpfulnessDenominator\",\n",
        "        \"Time\",\n",
        "        \"Summary\",\n",
        "    ],\n",
        "    inplace=True,\n",
        ")\n",
        "\n",
        "# make sentiment: 4,5 = positive, 1,2,3 = negative\n",
        "st = []\n",
        "count = 0\n",
        "for s in list(df[\"Score\"].values):\n",
        "    if s == 5 or s == 4:\n",
        "        st.append(1)\n",
        "        count += 1\n",
        "    else:\n",
        "        st.append(0)\n",
        "    pass\n",
        "df[\"st\"] = st\n",
        "print(\"positive:\", count)\n",
        "\n",
        "# max_len of the reviews\n",
        "max_len = 0\n",
        "for t in list(df[\"Text\"].values):\n",
        "    temp = t.split()\n",
        "    max_len = max(max_len, len(temp))\n",
        "    pass\n",
        "print(\"the max length is:\", max_len)\n",
        "\n",
        "# lower the case, remove the punctuations\n",
        "res = []\n",
        "for t in list(df[\"Text\"].values):\n",
        "    t = t.lower()\n",
        "\n",
        "    for p in punctuation:\n",
        "        t = t.replace(p, \"\")\n",
        "\n",
        "    res.append(t)\n",
        "    pass\n",
        "\n",
        "\n",
        "print(\"review example:\", res[0])\n",
        "df[\"Text\"] = res\n",
        "\n",
        "# encoding\n",
        "hist = {}\n",
        "for t in list(df[\"Text\"].values):\n",
        "    temp = t.split()\n",
        "    for word in temp:\n",
        "        hist[word] = hist.get(word, 0) + 1\n",
        "        pass\n",
        "    pass\n",
        "\n",
        "ind = 1\n",
        "word2ind, ind2word = {}, {}\n",
        "\n",
        "for k, v in sorted(hist.items(), key=lambda x: x[1], reverse=True):\n",
        "    word2ind[k] = ind\n",
        "    ind2word[ind] = k\n",
        "    ind += 1\n",
        "    pass\n",
        "\n",
        "\n",
        "def encoder(df):\n",
        "    X = []\n",
        "    for t in list(df[\"Text\"].values):\n",
        "        temp = []\n",
        "        words = t.split()\n",
        "        for word in words:\n",
        "            temp.append(word2ind[word])\n",
        "            pass\n",
        "        X.append(temp)\n",
        "        pass\n",
        "    return X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SVIbX0N74gw",
        "outputId": "f0674f8e-1b16-4186-f85f-b719dd89e434"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive: 7616\n",
            "the max length is: 1513\n",
            "review example: i have bought several of the vitality canned dog food products and have found them all to be of good quality the product looks more like a stew than a processed meat and it smells better my labrador is finicky and she appreciates this product better than  most\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train, test, validation split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=66)\n",
        "\n",
        "print(\"train shape is: {}\".format(train.shape))\n",
        "print(\"test shape is: {}\".format(test.shape))\n",
        "print(\n",
        "    \"The train set contains {:.2f}% positive reviews\".format(train[\"st\"].mean() * 100)\n",
        ")\n",
        "print(\"The test set cintains {:.2f}% positive reviews\".format(test[\"st\"].mean() * 100))\n",
        "\n",
        "train, validation = train_test_split(train, test_size=0.2, random_state=66)\n",
        "\n",
        "# get X for every dataset\n",
        "X_train = encoder(train)\n",
        "X_val = encoder(validation)\n",
        "X_test = encoder(test)\n",
        "\n",
        "# padding and truncate\n",
        "def padding_trun(max_feature, X):\n",
        "    new_X = np.zeros((len(X), max_feature), dtype=int)\n",
        "    for i, x in enumerate(X):\n",
        "        if len(x) > max_feature:\n",
        "            new_X[i, :] = np.array(x[:max_feature], dtype=int)\n",
        "        elif len(x) < max_feature:\n",
        "            temp = copy.deepcopy(x)\n",
        "            while len(temp) != max_feature:\n",
        "                temp.insert(0, 0)\n",
        "                pass\n",
        "            new_X[i, :] = np.array(temp, dtype=int)\n",
        "        else:\n",
        "            new_X[i, :] = np.array(x, dtype=int)\n",
        "        pass\n",
        "    return new_X\n",
        "\n",
        "\n",
        "max_feature = 256\n",
        "X_train = padding_trun(max_feature, X_train)\n",
        "y_train = np.array(train[\"st\"].values, dtype=int)\n",
        "\n",
        "X_test = padding_trun(max_feature, X_test)\n",
        "y_test = np.array(test[\"st\"].values, dtype=int)\n",
        "\n",
        "X_val = padding_trun(max_feature, X_val)\n",
        "y_val = np.array(validation[\"st\"].values, dtype=int)\n",
        "\n",
        "\n",
        "# convert to tensor\n",
        "train_data = TensorDataset(torch.from_numpy(X_train), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(X_val), torch.from_numpy(y_val))\n",
        "test_data = TensorDataset(torch.from_numpy(X_test), torch.from_numpy(y_test))\n",
        "\n",
        "# build dataloaders\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
        "valid_loader = DataLoader(valid_data, shuffle=False, batch_size=batch_size, drop_last=True)\n",
        "test_loader = DataLoader(test_data, shuffle=False, batch_size=batch_size, drop_last=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F6pfML118A3P",
        "outputId": "c1c31bde-4a2d-44ec-8818-2bddbaff081f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train shape is: (8000, 3)\n",
            "test shape is: (2000, 3)\n",
            "The train set contains 75.94% positive reviews\n",
            "The test set cintains 77.05% positive reviews\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        word_voc_count,\n",
        "        output_size,\n",
        "        embedding_dim,\n",
        "        hidden_dim,\n",
        "        layer_num,\n",
        "        drop_prob,\n",
        "    ):\n",
        "    # inputs -> embedding layer -> LSTM -> fully connected layer -> sigmoid -> predictions\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_num = layer_num\n",
        "        self.output_size = output_size\n",
        "\n",
        "        # embedding layer\n",
        "        self.embedding = nn.Embedding(word_voc_count, embedding_dim)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(\n",
        "            embedding_dim, hidden_dim, layer_num, dropout=drop_prob, batch_first=True\n",
        "        )\n",
        "\n",
        "        # dropout\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "\n",
        "        # fully connected layer using linear function\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "        # sigmoid activation function\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        # For prediction\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        embeds = self.embedding(x)\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sigmoid_out = self.sigmoid(out)\n",
        "\n",
        "        sigmoid_out = sigmoid_out.view(batch_size, -1)\n",
        "        sigmoid_out = sigmoid_out[:, -1]\n",
        "\n",
        "        return sigmoid_out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "\n",
        "        weight = next(self.parameters()).data\n",
        "\n",
        "        if device_gpu:\n",
        "            hidden = (\n",
        "                weight.new(self.layer_num, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "                weight.new(self.layer_num, batch_size, self.hidden_dim).zero_().cuda(),\n",
        "            )\n",
        "        else:\n",
        "            hidden = (\n",
        "                weight.new(self.layer_num, batch_size, self.hidden_dim).zero_(),\n",
        "                weight.new(self.layer_num, batch_size, self.hidden_dim).zero_(),\n",
        "            )\n",
        "\n",
        "        return hidden\n",
        "\n",
        "\n",
        "word_voc_count = len(word2ind) + 1\n",
        "output_size = 1\n",
        "embedding_dim = 512\n",
        "hidden_dim = 256\n",
        "layer_num = 2\n",
        "drop_prob = 0.35\n",
        "model = LSTM(word_voc_count, output_size, embedding_dim, hidden_dim, layer_num, drop_prob)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co2go1mm8HzJ",
        "outputId": "c0773862-57d2-4004-c189-f7210ad97ef3"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (embedding): Embedding(24896, 512)\n",
            "  (lstm): LSTM(512, 256, num_layers=2, batch_first=True, dropout=0.35)\n",
            "  (dropout): Dropout(p=0.2, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "# GPU\n",
        "device_gpu = True\n",
        "learning_rate = 0.001\n",
        "\n",
        "# binary cross entropy for the binary classification problems\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# achieves acc of 82% which outperforms SGD with acc of 75%\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "count = 0\n",
        "print_epoch = 100\n",
        "clip = 5\n",
        "\n",
        "if device_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "model.train()\n",
        "for e in range(epochs):\n",
        "    hid = model.init_hidden(batch_size)\n",
        "\n",
        "    for predictors, lab_y in train_loader:\n",
        "        count += 1\n",
        "\n",
        "        if device_gpu:\n",
        "            predictors, lab_y = predictors.cuda(), lab_y.cuda()\n",
        "\n",
        "        hid = tuple([each.data for each in hid])\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        predictors = predictors.type(torch.LongTensor)\n",
        "        output, hid = model(predictors.cuda(), hid)\n",
        "\n",
        "        loss = criterion(output.squeeze(), lab_y.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "\n",
        "        if count % print_epoch == 0:\n",
        "            val_hid = model.init_hidden(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for predictors, lab_y in valid_loader:\n",
        "                val_hid = tuple([each.data for each in val_hid])\n",
        "\n",
        "                if device_gpu:\n",
        "                    predictors, lab_y = predictors.cuda(), lab_y.cuda()\n",
        "\n",
        "                predictors = predictors.type(torch.LongTensor)\n",
        "\n",
        "                if (predictors.shape[0], predictors.shape[1]) != (batch_size, max_feature):\n",
        "                    continue\n",
        "\n",
        "                output, val_hid = model(predictors.cuda(), val_hid)\n",
        "                val_loss = criterion(output.squeeze(), lab_y.float())\n",
        "\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "            model.train()\n",
        "            print(\n",
        "                \"Epochs: {}, \".format(e + 1),\n",
        "                \"Steps: {}, \".format(count),\n",
        "                \"Loss: {:.6f}, \".format(loss.item()),\n",
        "            )"
      ],
      "metadata": {
        "id": "hOB9nm6_8OIs",
        "outputId": "544ebeef-fc7e-4da6-b1bc-ab828b43865e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1,  Steps: 100,  Loss: 0.551388, \n",
            "Epochs: 2,  Steps: 200,  Loss: 0.383713, \n",
            "Epochs: 3,  Steps: 300,  Loss: 0.168112, \n",
            "Epochs: 4,  Steps: 400,  Loss: 0.087152, \n",
            "Epochs: 4,  Steps: 500,  Loss: 0.031963, \n",
            "Epochs: 5,  Steps: 600,  Loss: 0.025938, \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_testing = list()\n",
        "correct_count = 0\n",
        "hid = model.init_hidden(batch_size)\n",
        "\n",
        "res = []\n",
        "\n",
        "model.eval()\n",
        "\n",
        "for predictors, lab_y in test_loader:\n",
        "\n",
        "    hid = tuple([each.data for each in hid])\n",
        "\n",
        "    if device_gpu:\n",
        "        predictors, lab_y = predictors.cuda(), lab_y.cuda()\n",
        "\n",
        "    predictors = predictors.type(torch.LongTensor)\n",
        "    output, hid = model(predictors.cuda(), hid)\n",
        "\n",
        "    test_loss = criterion(output.squeeze(), lab_y.float())\n",
        "    loss_testing.append(test_loss.item())\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "\n",
        "    correct_tensor = pred.eq(lab_y.float().view_as(pred))\n",
        "\n",
        "    if device_gpu:\n",
        "        correct = np.squeeze(correct_tensor.cpu().numpy())\n",
        "    else:\n",
        "        correct = np.squeeze(correct_tensor.numpy())\n",
        "\n",
        "    res += list(correct)\n",
        "    correct_count += np.sum(correct)\n",
        "    \n",
        "acc_testing = correct_count / len(test_loader.dataset)\n",
        "\n",
        "print(\"Loss for the test dataset:\", np.mean(loss_testing))\n",
        "print(\"Accuracy for the test dataset:\", acc_testing)"
      ],
      "metadata": {
        "id": "DO0928Ua8Ylm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e424417c-d90c-4564-9632-4c4a3f6b7b4f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss for the test dataset: 0.6717493239790201\n",
            "Accuracy for the test dataset: 0.835\n",
            "2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# output file for cases analysis\n",
        "test[\"res\"] = res\n",
        "test.to_csv('/content/drive/My Drive/testdata_real.csv')"
      ],
      "metadata": {
        "id": "dgnYlOggNvwA"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}